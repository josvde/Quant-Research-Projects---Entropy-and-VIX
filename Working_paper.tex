\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{fancyhdr}

\pagestyle{fancy}
\fancyhead{}
\fancyfoot{}
\fancyhead[L]{\slshape \MakeUppercase{Quantitative Research Projects}}
\fancyhead[R]{\slshape {BETA SIGMA CLUB}}
\fancyfoot[C]{\thepage}


\begin{document}

\begin{titlepage}
\begin{center}
\vspace*{1cm}
\vfill
\line(1,0){400}\\[1mm]
\huge{\textbf{Quantitative Research Projects}}\\[3mm]
\Large{\textbf{By BETA SIGMA CLUB}}\\[1mm]
\line(1,0){400}\\[3mm]
\vfill
\today

\end{center}
\end{titlepage}

\section{Abstract and Keywords}
\textbf{Abstract:} Comparision of the standard measure for stock market volatility, standard deviation, with an alternative measure, entropy. \\

\noindent \textbf{Keywords:}
\begin{itemize}
\item Shannon Entropy
\item Tsallis Entropy
\item Alternative stock market volatility measures
\item Volatility measurement
\item Econophysics
\end{itemize}

\noindent \textbf{Questions:}
\begin{itemize}
\item Do we include \textbf{time variant} volatility measures like the ARCH (Autoregressive Conditional Heteroscedastic) model in our comparison?
\end{itemize}

\newpage
\section{Introduction}
Before diving deep into Entropy's application as a measure of volatility, we need to introduce how we can use physics in financial markets.
\subsection{Econophysics}
Econophysics is an interdisciplinary research field where physics is used to solve problems in Economics. The applications of physics, and more particularly statistical physics, in economics started being used as people realised that economics and markets were not efficient. Thus, a more statistical approach was needed. The stochastic processes and nonlinear dynamics from statistical physics later founded the basis of statistical finance. In this paper, we are going to handle entropy as a new, and more powerful stock market volatility metric. 
\subsection{Information Theory}
The mathematical treatment of the ideas, parameters, and regulations regulating the transmission of messages through communication networks is known as information theory. Around the middle of the 20th century, Claude Shannon founded it. It has since developed into a robust area of mathematics.
Some people consider information theory to be a subset of probability theory since the techniques utilised in it are probabilistic in nature. The information included in a message describing one of a set of potential occurrences quantifies the number of symbols required to best encode that event. Information theory still contributes to findings in statistics and statistical mechanics.

\section{Physical Aspects of Entropy}
Entropy finds its origin in thermodynamics, a branch of physics, and is often described as the rate of disorder randomness, or uncertainty. Ludwig Boltzmann, an Austrian physicist defined entropy as the number of possible microscopic states of individual atoms and molecules of a system. With this, he introduced the concept of statistical disorder and probability distributions. Boltzmann's famous formula for entropy (denoted by S) is defined as:
\begin{equation*}
    S = k_{B} \ln W
\end{equation*}
Where W is the number of possible microscopic states corresponding to the macroscopic state of a system.
\begin{equation*}
    W = N! \ \prod_{i}^{} \frac{1}{N_{i}!}
\end{equation*}
and N the amount of \textit{identical} particles. Although entropy is a strictly statistical mechanics concept, it has many applications in other fields. For the sake of this paper, we will mention entropy applications in information theory.

\subsection{Shannon Entropy}
Shannon entropy comes from information theory and is also defined as a rate of uncertainty. Shannon entropy is more precise when we are including more variables. In \eqref{eq1} we defined $W$ as the number of possible microscopic states, this is always correct, if and only if the probability that a microscopic state occurs is equally distributed. Although, in reality, it will not always be the case, think for instance at an unfair dice, one number could occur more/less than another. Shannon Entropy is defined as:
\begin{equation*}
H(X) = - \sum_{x}^{} p_{i}\log_b (p_{i})
\end{equation*}

\section{Mathematical Aspects of Entropy: Louis and Hamza}
\section{Financial Engineering section: Hamza, Edoardo and Jos}


\end{document}